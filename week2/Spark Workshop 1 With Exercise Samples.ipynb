{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark \n",
    "\n",
    "https://spark.apache.org/\n",
    "\n",
    "## Spark Documentation Guide\n",
    "\n",
    "https://spark.apache.org/docs/latest/\n",
    "\n",
    "### API Docs\n",
    "#### Python\n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "#### Scala\n",
    "https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\n",
    "\n",
    "### Spark Standalone Guide\n",
    "https://spark.apache.org/docs/latest/spark-standalone.html\n",
    "\n",
    "\n",
    "## Books and Other Good References\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/  \n",
    "https://data-flair.training/blogs/spark-tutorial/  \n",
    "https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #2 Exercise Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Session Outline:-\n",
    "1. Data Loading\n",
    "    * textFile\n",
    "    * parallelize\n",
    "    * from Master in a distributed environment (Week 3)\n",
    "2. MapReduce\n",
    "    * transformation & actions\n",
    "        * map & collect\n",
    "        * filter & count\n",
    "        * flatMap & take\n",
    "        * filter & reduce\n",
    "        * union, intersection\n",
    "        * join, leftOuterJoin, cartesian, cogroup\n",
    "        * reduceByKey & collect\n",
    "3. Config\n",
    "    * using external files (Week 3)\n",
    "    * using API\n",
    "    * Spark Session (discussed with Spark SQL)\n",
    "4. Partitions\n",
    "    * fill it with random data\n",
    "    * find/use the partitions    \n",
    "5. Spark SQL\n",
    "    * Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to remember:- \n",
    "* Learn your hardware configuration like RAM, CPU cores, etc.\n",
    "* All Spark functions will be given along with the questions, you have to fill Spark function with their respective parameters <br /> and write the corresponding Scala or Python Logic\n",
    "* This is a practice session, so no scores are calculated \n",
    "* For quicker programming, we will use the shell environment today\n",
    "* If your IDE configurations aren't working, approach us after the exercise session\n",
    "* Ofcourse, Solutions will be provided for these questions after this exercise session\n",
    "* If you are already familiar with the contents listed above, go ahead in learning Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration:-\n",
    "\n",
    "https://github.com/shathil/BigDataExercises\n",
    "\n",
    "From the above url, download the git repository as a zip file. From the dowloaded zip package, extract a folder called resources and place it inside your spark 2.x folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* In the Spark shell, the SparkContext object is already initialized and available as sc. \n",
    "* Now load a \"textFile\" 'pg1567.txt' from the resources folder into an RDD named textRDD\n",
    "* Use the path as 'resources/pg157.txt' to load the textFile. \n",
    "* Now transform the textRDD into a map and split each line into words(use space as delimiter).\n",
    "* Finally count the number of words using \"count\"\n",
    "\n",
    "#### Spark functions needed:-\n",
    "* textFile\n",
    "* map\n",
    "* count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Create an Scala array or a Python list containg any 5 numbers and store it into a variable called \"numbers\"\n",
    "* Use Spark \"parallelize\" function on the numbers variable and store the resultant RDD as numRDD\n",
    "* Now transform the numRDD into a map by finding the squares of all numbers, store it as numMap\n",
    "* Finally collect the squared numbers using \"collect\"\n",
    "\n",
    "#### Spark functions needed:-\n",
    "* parallelize\n",
    "* map\n",
    "* collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Generate random 10000 numbers and store it in a variable called \"numbers\"\n",
    "* parallelize the \"numbers\" and store the result into \"numRDD\"\n",
    "* Now filter the positive numbers from numRDD and,\n",
    "* Count the positive numbers\n",
    "\n",
    "#### Spark functions needed:-\n",
    "* parallelize\n",
    "* filter\n",
    "* count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Now use flatMap on \"numRDD\" and find the squares of all numbers and store it as \"squaredRDD\"\n",
    "* Try printing first element in squaredRDD using a Spark action called \"first\"\n",
    "* Try printing first 10 element in squaredRDD using a Spark action called \"take\"\n",
    "\n",
    "#### Spark functions needed:-\n",
    "* flatMap\n",
    "* first\n",
    "* take\n",
    "\n",
    "Hint:- flatMap-ed RDD might not work like map or filter functions. Calling actions on these flatMap-ed RDD directly, will definitely throw an error. Can you guess why?. If you have already experienced this, and solved it, congrats!. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* There are two ways to perform the logic inside the transformations: call a function, or an inline function\n",
    "* Try learning both ways \n",
    "* Combine question 3 and 4, Now filter all negative values, square them and print first 20 of them.\n",
    "\n",
    "### Spark functions needed:-\n",
    "* filter\n",
    "* use either map or flatMap\n",
    "* take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Filter all positive values from numRDD into \"filteredRDD\"\n",
    "* Now reduce the filteredRDD (finding the sum of all positive values) and collect it \n",
    "* Also now collect the filteredRDD and the find the sum for the collected result\n",
    "* Compare the time difference between a native sum function and using a native sum function inside a Spark function. Did you find any difference?. No?\n",
    "* Okay now generate a million(1000000) random numbers and store it into numRDD and repeat above four steps\n",
    "* Still dont feel the significance, try increasing the count to 10 million ans repeat first four steps\n",
    "\n",
    "### Spark functions needed:-\n",
    "* reduce\n",
    "* collect\n",
    "\n",
    "## Note: \n",
    "If your hardware is below 8GB and has other processes running , stop before 1 million. Try this when no other processes are running. \n",
    "\n",
    "## Hint:\n",
    "* There is an easy and graphical way to watch the time taken to complete the process.\n",
    "* Go to your browser, and type localhost:4040 and then hit Enter\n",
    "* What you see is Spark Web UI, we will cover this in detail during Cluster programming. \n",
    "* Did you notice the old Spark actions all listed there! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "* This is an easy question\n",
    "* Create two RDDs with 5 numbers each called \"num1\" and \"num2\"\n",
    "* Find the \"union\" and \"intersection\" of the two RDD's\n",
    "* Told you it's easy :)\n",
    "\n",
    "### Spark functions needed:-\n",
    "* union\n",
    "* intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "* This is also a simple one :)\n",
    "* I have given the RDD set 1 and 2 below. Try all listed Spark operations below on each set.\n",
    "\n",
    "### Spark functions needed:-\n",
    "* join\n",
    "* leftOuterJoin\n",
    "* rightOuterJoin\n",
    "* cartesian\n",
    "\n",
    "### RDD set 1:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining with just values\n",
    "rdd1 =  sc.parallelize([(\"foo\", 1), (\"bar\", 2), (\"baz\", 3)])\n",
    "rdd2 =  sc.parallelize([(\"foo\", 4), (\"bar\", 5), (\"bar\", 6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD set 2:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining with just list items\n",
    "words1 = sc.parallelize([\"Hello\", \"Human\"])\n",
    "words2 = sc.parallelize([\"world\", \"all\", \"you\", \"Mars\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "* Let's count the words in all works of william shakespeare!\n",
    "* Load the 'textFile' named 'pg100.txt' from the resources folder into poemRDD\n",
    "* Use function chain on all these following actions:-\n",
    "    * Transform the poemRDD into a flatMap and split the words using space(' ') as a delimiter\n",
    "    * Now convert the flatMap into a map and convert each word into a tuple with two values (word, 1)\n",
    "    * Now reduceByKey and add all the values. Here key is the word\n",
    "    * Collect the result\n",
    "\n",
    "### Spark functions needed:-\n",
    "* flatMap\n",
    "* map\n",
    "* reduceByKey\n",
    "* collect\n",
    "\n",
    "#### Self Exercise:- \n",
    "Think about displaying words with highest values in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "* Try find all configuration for the default shell SparkContext. Whcih function to use?.\n",
    "* To create a new config, stop the running SparkContext first\n",
    "* Now create a new Conf Object with an AppName, and Master\n",
    "* Check if \"spark.dynamicAllocation.enabled\" is true, if False, execute the next step\n",
    "* Set the \"spark.dynamicAllocation.enabled\" to be \"true\" for the Conf\n",
    "* Pass the newly created conf as a parameter to the SparkContext constructor and assign the new SparkContext to sc\n",
    "\n",
    "### Hint:-\n",
    "For finding the existing shell sc configuration,\n",
    "\n",
    "http://stackoverflow.com/questions/30560241/is-it-possible-to-get-the-current-spark-context-settings-in-pyspark\n",
    "\n",
    "Finding \"spark.dynamicAllocation.enabled\" status,\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-dynamic-allocation.html\n",
    "\n",
    "For setting dynamicAllocation,\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkConf.html\n",
    "\n",
    "\n",
    "### Note:- \n",
    "So did you get curious why we set the Dynamic Allocation in the config?. Have you found out where it is useful?.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "* Now shall we try partitions with Question 6. \n",
    "* parallelize function takes two parameters, the first is the data object, the second is 'numSlices'\n",
    "* Fill the numSlices value <= your number of CPU cores\n",
    "* Now observe the time difference in executing steps 1 to 4 of Question 6\n",
    "* If feasible, try with 1 to 10 million random numbers\n",
    "* can you observe that finding optimum 'numSlices' value is itself an Optimization problem?.\n",
    "* Try using 'getNumPartitions' on any of the RDD's before and after increasing the number of partitions\n",
    "\n",
    "### Spark functions needed:-\n",
    "* parallelize\n",
    "* reduce\n",
    "* collect\n",
    "* getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "* Another easy one, this is just a copy/paste of Question 9\n",
    "* Just with the textFile method, provide an addition argument 'minPartitions' and observe the time difference in evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session\n",
    "\n",
    "* We are going to create a Spark Session. Why this is different from SparkContext and SparkConf?. \n",
    "* Store the Spark session object into a variable called 'spark'\n",
    "\n",
    "### Hint : \n",
    "https://docs.databricks.com/spark/latest/gentle-introduction/sparksession.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 12\n",
    "\n",
    "* Loading data from JSON\n",
    "* using 'spark' variable, read 'peopl.json' from resources folder. Store it into a variable called 'df'\n",
    "* 'df' is a dataFrame, view it's contents using \"show\" method\n",
    "* use 'select' function on 'df' to choose a single column and display it using 'show' method\n",
    "* use 'filter' function on 'df' to execute a filter condition like 'df[age] > 18' and display the results using 'show'\n",
    "\n",
    "### Self Exercise:-\n",
    "* Learn to create Temp View\n",
    "* Learn to create Global Temp View\n",
    "* Find the difference between Global temporary view and temporary view. And check it using an example.\n",
    "\n",
    "\n",
    "#### For More SQL Exercises Refer,\n",
    "For Python,\n",
    "https://github.com/apache/spark/tree/master/examples/src/main/python/sql\n",
    "\n",
    "For Scala,\n",
    "https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Question 13 - TODO Week 3\n",
    "\n",
    "* Loading data from text file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
